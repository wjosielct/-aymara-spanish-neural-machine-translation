# Traductor Autom치tico Neuronal Aymara-Espa침ol: Un Enfoque para Pares de Bajos Recursos

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.5%2B-ee4c2c)
![License](https://img.shields.io/badge/License-MIT-green)

Este repositorio contiene la implementaci칩n oficial del c칩digo para el proyecto del a침o 2026 del curso de NLP de la Maestr칤a en Inteligencia Artificial (Universidad Nacional de Ingenier칤a, Lima - Per칰): **"Traductor Autom치tico Neuronal para el Par Aymara-Espa침ol: Un Enfoque para Pares de Bajos Recursos"**.

El proyecto implementa dos configuraciones de la arquitectura Transformer (Est치ndard y 'Deep Encoder / Shallow Decoder') optimizada para realizar traducci칩n autom치tica en un escenario de bajos recursos (*Low-Resource NMT*), utilizando un corpus paralelo construido a partir de textos b칤blicos y literatura en PDF.

## 游늭 Estructura del Repositorio

El c칩digo est치 organizado seg칰n el flujo de trabajo del proyecto:

### 1. Adquisici칩n de Datos
- **`scraping_bible.ipynb`**: Script de *web scraping* utilizando `BeautifulSoup` y `requests` para extraer vers칤culos paralelos (Aymara/Espa침ol) de la web Bible.com.

### 2. Preprocesamiento y Tokenizaci칩n
- **`preprocessing_bible.ipynb`**: Normalizaci칩n de caracteres unicode, limpieza de puntuaci칩n y alineamiento de vers칤culos extra칤dos.
- **`preprocessing_book.ipynb`**: Script de extracci칩n de texto desde un archivo PDF (`book.pdf`) utilizando `pdfplumber`. Incluye l칩gica de limpieza, reconstrucci칩n de oraciones y alineamiento de di치logos.
- **`tokenization.ipynb`**: Entrenamiento de un tokenizador BPE (Byte-Pair Encoding) utilizando `SentencePiece`. Generaci칩n de vocabulario y archivos tokenizados (`.ids`).

### 3. Modelado y Entrenamiento
- **`training_AymaraToSpanish_Standard.ipynb`, `training_SpanishToAymara_Standard.ipynb`, `training_AymaraToSpanish_DeepEncoder-ShallowDecoder.ipynb`, `training_SpanishToAymara_DeepEncoder-ShallowDecoder.ipynb`**: Implementaci칩n en PyTorch del modelo Transformer.
    - Incluye clases para `Dataset`, `DataLoader` y el bucle de entrenamiento/validaci칩n.
    - Implementa *Beam Search Decoding* para la inferencia.
    - Calcula m칠tricas BLEU utilizando `sacrebleu`.

## 游 Instalaci칩n y Requisitos

Para ejecutar los notebooks, aseg칰rate de instalar las siguientes dependencias:

```bash
pip3 install torch --index-url https://download.pytorch.org/whl/cu121
pip install sentencepiece sacrebleu pandas numpy matplotlib
pip install beautifulsoup4 requests pdfplumber
```

## 游 Arquitectura del Modelo

El proyecto implementa y compara dos variantes de la arquitectura Transformer para optimizar el rendimiento en un escenario de bajos recursos:

| Configuraci칩n | Encoder Layers | Decoder Layers | D_Model | Dropout | Notas |
| :--- | :---: | :---: | :---: | :---: | :--- |
| **Baseline** | 6 | 6 | 512 | 0.1 | Arquitectura est치ndar (Vaswani et al., 2017). |
| **Deep-Shallow** | 12 | 2 | 512 | 0.3 | **Propuesta**. Utiliza *Pre-Normalization* (`norm_first=True`) y un Encoder profundo para capturar la compleja morfolog칤a aglutinante del Aymara, junto con un Decoder superficial para evitar alucinaciones. |

## 游늵 Resultados Preliminares

La evaluaci칩n del modelo se realiz칩 utilizando la m칠trica **BLEU** sobre un conjunto de prueba (*Test Set*) reservado (aprox. 8,000 oraciones), aplicando *Beam Search Decoding* con un ancho de 5.

| Direcci칩n de Traducci칩n | BLEU Score | Observaciones |
| :--- | :---: | :--- |
| **Aymara $\rightarrow$ Espa침ol** | **28.95** | Resultado competitivo para un escenario de bajos recursos. El modelo logra generalizar bien hacia la gram치tica del espa침ol. |
| **Espa침ol $\rightarrow$ Aymara** | **13.37** | El menor puntaje refleja la complejidad de generar una lengua **aglutinante**. El modelo enfrenta el desaf칤o de predecir la combinaci칩n exacta de sufijos aymaras a partir del espa침ol. |

> ## 丘멆잺 Disponibilidad de los Datos y Copyright

**Importante:** Este repositorio de c칩digo **NO contiene los datasets crudos** (textos b칤blicos originales ni el archivo PDF del libro) debido a restricciones de propiedad intelectual (*Copyright*).

### 游닌 Obtener el Dataset Limpio
El dataset procesado (alineado, limpio y dividido en *train/valid/test*) est치 disponible 칰nicamente para **fines de investigaci칩n acad칠mica** a trav칠s de los siguientes repositorios:

- **Hugging Face:** [https://huggingface.co/datasets/wjosielct/aymara-spanish-parallel-corpus]
- **Zenodo:** [https://doi.org/10.5281/zenodo.18193320]

### 丘뒲잺 Descargo de Responsabilidad (Disclaimer)
Los scripts de adquisici칩n de datos (`scraping_bible.ipynb` y `preprocessing_book.ipynb`) se proporcionan con fines de reproducibilidad t칠cnica.
> *The texts processed in this project are derived from copyrighted sources. This repository claims no ownership over the original content. The processed data is distributed solely for **non-commercial academic research** aimed at preserving the Aymara language.*

## 游닇 Citaci칩n

Si utilizas este c칩digo, los modelos entrenados o la metodolog칤a descrita en tu investigaci칩n, por favor cita el trabajo de la siguiente manera:

```bibtex
@mastersthesis{AymaraNMT2026,
  author  = {Corbera Terrones, Josiel},
  title   = {Traductor Autom치tico Neuronal para el Par Aymara-Espa침ol: Un Enfoque para Pares de Bajos Recursos},
  school  = {Universidad Nacional de Ingenier칤a (UNI)},
  year    = {2026},
  address = {Lima, Per칰},
  note    = {Disponible en GitHub}
}
```
## 游늯 Licencia

El c칩digo fuente de este proyecto se distribuye bajo la licencia **MIT**. Esto permite su uso, modificaci칩n y distribuci칩n libre para fines acad칠micos y comerciales, siempre que se mantenga el reconocimiento al autor original.

Consulta el archivo [LICENSE](LICENSE) para ver los t칠rminos completos.

> **Nota:** Esta licencia aplica 칰nicamente al **software** (scripts, notebooks). Los datos procesados y los modelos derivados est치n sujetos a las restricciones de copyright y licencias no comerciales (*CC-BY-NC-SA*) mencionadas en la secci칩n de "Disponibilidad de los Datos".
